{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommender Systems using Autoencoder\n",
    "-- predicting the rating of a movies on the scale of 5 not visited by user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('DataSet/ml-1m/movies.dat', sep = '::', header = None, engine= 'python', \n",
    "                     encoding='latin-1')\n",
    "user = pd.read_csv('DataSet/ml-1m/users.dat', sep = '::', header = None, engine= 'python', \n",
    "                     encoding='latin-1')\n",
    "rating = pd.read_csv('DataSet/ml-1m/ratings.dat', sep = '::', header = None, engine= 'python', \n",
    "                     encoding='latin-1')\n",
    "train = pd.read_csv('DataSet/ml-100k/u1.base', sep= '\\t')\n",
    "test = pd.read_csv('DataSet/ml-100k/u1.test', sep= '\\t')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.array(train, dtype='int')\n",
    "test = np.array(test, dtype='int')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of users and movies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_users = int(max(max(train[:, 0]), max(test[:,0])))\n",
    "nb_movies = int(max(max(train[:, 1]), max(test[:,1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(data):\n",
    "    new_df = []\n",
    "    for id_users in range(1, nb_users + 1):\n",
    "        id_movies = data[:, 1][data[:, 0] == id_users]\n",
    "        id_rating = data[:, 2][data[:, 0] == id_users]\n",
    "        ratings = np.zeros(nb_movies)\n",
    "        ratings[id_movies-1] = id_rating\n",
    "        new_df.append(list(ratings))\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = convert(train)\n",
    "test = convert(test)\n",
    "train = torch.FloatTensor(train)\n",
    "test = torch.FloatTensor(test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Autoencoder\n",
    "\n",
    "--we are making stacked Autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inheritance from a parent class \n",
    "class SAE(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(SAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(nb_movies, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        self.fc3 = nn.Linear(10, 20)\n",
    "        self.fc4 = nn.Linear(20, nb_movies)\n",
    "        self.activation = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = SAE() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = optim.RMSprop(ae.parameters(), lr=0.01, weight_decay= 0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch : 1 || Loss : 1.7708677053451538\n",
      " Epoch : 2 || Loss : 1.0965454578399658\n",
      " Epoch : 3 || Loss : 1.053270697593689\n",
      " Epoch : 4 || Loss : 1.0383684635162354\n",
      " Epoch : 5 || Loss : 1.0308600664138794\n",
      " Epoch : 6 || Loss : 1.02657151222229\n",
      " Epoch : 7 || Loss : 1.0235885381698608\n",
      " Epoch : 8 || Loss : 1.0220330953598022\n",
      " Epoch : 9 || Loss : 1.0206735134124756\n",
      " Epoch : 10 || Loss : 1.0194190740585327\n",
      " Epoch : 11 || Loss : 1.018898367881775\n",
      " Epoch : 12 || Loss : 1.0183336734771729\n",
      " Epoch : 13 || Loss : 1.0178663730621338\n",
      " Epoch : 14 || Loss : 1.0175273418426514\n",
      " Epoch : 15 || Loss : 1.0173407793045044\n",
      " Epoch : 16 || Loss : 1.0169599056243896\n",
      " Epoch : 17 || Loss : 1.0168157815933228\n",
      " Epoch : 18 || Loss : 1.0161656141281128\n",
      " Epoch : 19 || Loss : 1.016269326210022\n",
      " Epoch : 20 || Loss : 1.0159651041030884\n",
      " Epoch : 21 || Loss : 1.016068458557129\n",
      " Epoch : 22 || Loss : 1.0158530473709106\n",
      " Epoch : 23 || Loss : 1.0159090757369995\n",
      " Epoch : 24 || Loss : 1.015783429145813\n",
      " Epoch : 25 || Loss : 1.0153979063034058\n",
      " Epoch : 26 || Loss : 1.0157188177108765\n",
      " Epoch : 27 || Loss : 1.015380859375\n",
      " Epoch : 28 || Loss : 1.014601230621338\n",
      " Epoch : 29 || Loss : 1.0131131410598755\n",
      " Epoch : 30 || Loss : 1.0117788314819336\n",
      " Epoch : 31 || Loss : 1.009365439414978\n",
      " Epoch : 32 || Loss : 1.009927749633789\n",
      " Epoch : 33 || Loss : 1.0054961442947388\n",
      " Epoch : 34 || Loss : 1.0052424669265747\n",
      " Epoch : 35 || Loss : 1.0016688108444214\n",
      " Epoch : 36 || Loss : 0.9996618628501892\n",
      " Epoch : 37 || Loss : 0.9961515069007874\n",
      " Epoch : 38 || Loss : 0.9945311546325684\n",
      " Epoch : 39 || Loss : 0.992586612701416\n",
      " Epoch : 40 || Loss : 0.9923219680786133\n",
      " Epoch : 41 || Loss : 0.9867584109306335\n",
      " Epoch : 42 || Loss : 0.9884951114654541\n",
      " Epoch : 43 || Loss : 0.9835748076438904\n",
      " Epoch : 44 || Loss : 0.9833375811576843\n",
      " Epoch : 45 || Loss : 0.9793877005577087\n",
      " Epoch : 46 || Loss : 0.9814585447311401\n",
      " Epoch : 47 || Loss : 0.9790659546852112\n",
      " Epoch : 48 || Loss : 0.977598249912262\n",
      " Epoch : 49 || Loss : 0.9726321697235107\n",
      " Epoch : 50 || Loss : 0.9718441367149353\n",
      " Epoch : 51 || Loss : 0.9698985815048218\n",
      " Epoch : 52 || Loss : 0.967039942741394\n",
      " Epoch : 53 || Loss : 0.9651482701301575\n",
      " Epoch : 54 || Loss : 0.9639147520065308\n",
      " Epoch : 55 || Loss : 0.9621057510375977\n",
      " Epoch : 56 || Loss : 0.9599193334579468\n",
      " Epoch : 57 || Loss : 0.9593610167503357\n",
      " Epoch : 58 || Loss : 0.9577974677085876\n",
      " Epoch : 59 || Loss : 0.95661860704422\n",
      " Epoch : 60 || Loss : 0.9562459588050842\n",
      " Epoch : 61 || Loss : 0.9542504549026489\n",
      " Epoch : 62 || Loss : 0.9533090591430664\n",
      " Epoch : 63 || Loss : 0.9514333605766296\n",
      " Epoch : 64 || Loss : 0.9506560564041138\n",
      " Epoch : 65 || Loss : 0.9490067958831787\n",
      " Epoch : 66 || Loss : 0.9493454694747925\n",
      " Epoch : 67 || Loss : 0.9474563598632812\n",
      " Epoch : 68 || Loss : 0.9481987357139587\n",
      " Epoch : 69 || Loss : 0.9459718465805054\n",
      " Epoch : 70 || Loss : 0.9461644887924194\n",
      " Epoch : 71 || Loss : 0.944555401802063\n",
      " Epoch : 72 || Loss : 0.9447014927864075\n",
      " Epoch : 73 || Loss : 0.9433930516242981\n",
      " Epoch : 74 || Loss : 0.9439277648925781\n",
      " Epoch : 75 || Loss : 0.9417862892150879\n",
      " Epoch : 76 || Loss : 0.9424835443496704\n",
      " Epoch : 77 || Loss : 0.9407867193222046\n",
      " Epoch : 78 || Loss : 0.9415286183357239\n",
      " Epoch : 79 || Loss : 0.9399974346160889\n",
      " Epoch : 80 || Loss : 0.9399261474609375\n",
      " Epoch : 81 || Loss : 0.9385705590248108\n",
      " Epoch : 82 || Loss : 0.9392942786216736\n",
      " Epoch : 83 || Loss : 0.9382743835449219\n",
      " Epoch : 84 || Loss : 0.9387575387954712\n",
      " Epoch : 85 || Loss : 0.937237024307251\n",
      " Epoch : 86 || Loss : 0.9380974173545837\n",
      " Epoch : 87 || Loss : 0.9365871548652649\n",
      " Epoch : 88 || Loss : 0.9368162155151367\n",
      " Epoch : 89 || Loss : 0.9355403780937195\n",
      " Epoch : 90 || Loss : 0.9362389445304871\n",
      " Epoch : 91 || Loss : 0.9348077774047852\n",
      " Epoch : 92 || Loss : 0.9350957870483398\n",
      " Epoch : 93 || Loss : 0.9339134097099304\n",
      " Epoch : 94 || Loss : 0.9343675971031189\n",
      " Epoch : 95 || Loss : 0.9333547353744507\n",
      " Epoch : 96 || Loss : 0.9336432218551636\n",
      " Epoch : 97 || Loss : 0.9327444434165955\n",
      " Epoch : 98 || Loss : 0.9328755140304565\n",
      " Epoch : 99 || Loss : 0.931685745716095\n",
      " Epoch : 100 || Loss : 0.9321823120117188\n",
      " Epoch : 101 || Loss : 0.9309797286987305\n",
      " Epoch : 102 || Loss : 0.9314503073692322\n",
      " Epoch : 103 || Loss : 0.9306495189666748\n",
      " Epoch : 104 || Loss : 0.9313175082206726\n",
      " Epoch : 105 || Loss : 0.9297776818275452\n",
      " Epoch : 106 || Loss : 0.9305497407913208\n",
      " Epoch : 107 || Loss : 0.9294176697731018\n",
      " Epoch : 108 || Loss : 0.9296439290046692\n",
      " Epoch : 109 || Loss : 0.9287148714065552\n",
      " Epoch : 110 || Loss : 0.9293568134307861\n",
      " Epoch : 111 || Loss : 0.9282829165458679\n",
      " Epoch : 112 || Loss : 0.9290552139282227\n",
      " Epoch : 113 || Loss : 0.9275218844413757\n",
      " Epoch : 114 || Loss : 0.928767204284668\n",
      " Epoch : 115 || Loss : 0.9268608689308167\n",
      " Epoch : 116 || Loss : 0.9277263283729553\n",
      " Epoch : 117 || Loss : 0.9265770316123962\n",
      " Epoch : 118 || Loss : 0.9269612431526184\n",
      " Epoch : 119 || Loss : 0.9258489012718201\n",
      " Epoch : 120 || Loss : 0.926110029220581\n",
      " Epoch : 121 || Loss : 0.9251730442047119\n",
      " Epoch : 122 || Loss : 0.9254342913627625\n",
      " Epoch : 123 || Loss : 0.9245699048042297\n",
      " Epoch : 124 || Loss : 0.925009548664093\n",
      " Epoch : 125 || Loss : 0.923870861530304\n",
      " Epoch : 126 || Loss : 0.9244472980499268\n",
      " Epoch : 127 || Loss : 0.9235102534294128\n",
      " Epoch : 128 || Loss : 0.9237880706787109\n",
      " Epoch : 129 || Loss : 0.9230057597160339\n",
      " Epoch : 130 || Loss : 0.9232287406921387\n",
      " Epoch : 131 || Loss : 0.922481894493103\n",
      " Epoch : 132 || Loss : 0.9227762222290039\n",
      " Epoch : 133 || Loss : 0.9219983816146851\n",
      " Epoch : 134 || Loss : 0.9222891330718994\n",
      " Epoch : 135 || Loss : 0.9215348362922668\n",
      " Epoch : 136 || Loss : 0.921790599822998\n",
      " Epoch : 137 || Loss : 0.9210869073867798\n",
      " Epoch : 138 || Loss : 0.921431303024292\n",
      " Epoch : 139 || Loss : 0.9205886125564575\n",
      " Epoch : 140 || Loss : 0.9209160804748535\n",
      " Epoch : 141 || Loss : 0.9201943278312683\n",
      " Epoch : 142 || Loss : 0.9203951954841614\n",
      " Epoch : 143 || Loss : 0.91973477602005\n",
      " Epoch : 144 || Loss : 0.9199076890945435\n",
      " Epoch : 145 || Loss : 0.9192234873771667\n",
      " Epoch : 146 || Loss : 0.9194673299789429\n",
      " Epoch : 147 || Loss : 0.9188991189002991\n",
      " Epoch : 148 || Loss : 0.9192057251930237\n",
      " Epoch : 149 || Loss : 0.9185640811920166\n",
      " Epoch : 150 || Loss : 0.9189808964729309\n",
      " Epoch : 151 || Loss : 0.9181610345840454\n",
      " Epoch : 152 || Loss : 0.9185929894447327\n",
      " Epoch : 153 || Loss : 0.9178410768508911\n",
      " Epoch : 154 || Loss : 0.9184345006942749\n",
      " Epoch : 155 || Loss : 0.9176118969917297\n",
      " Epoch : 156 || Loss : 0.9179667830467224\n",
      " Epoch : 157 || Loss : 0.9172098636627197\n",
      " Epoch : 158 || Loss : 0.9177200794219971\n",
      " Epoch : 159 || Loss : 0.9168567061424255\n",
      " Epoch : 160 || Loss : 0.9171878695487976\n",
      " Epoch : 161 || Loss : 0.9165276885032654\n",
      " Epoch : 162 || Loss : 0.9171082377433777\n",
      " Epoch : 163 || Loss : 0.9161063432693481\n",
      " Epoch : 164 || Loss : 0.9166151881217957\n",
      " Epoch : 165 || Loss : 0.9157901406288147\n",
      " Epoch : 166 || Loss : 0.9163752794265747\n",
      " Epoch : 167 || Loss : 0.9156665205955505\n",
      " Epoch : 168 || Loss : 0.9158852100372314\n",
      " Epoch : 169 || Loss : 0.9152107238769531\n",
      " Epoch : 170 || Loss : 0.9158902764320374\n",
      " Epoch : 171 || Loss : 0.9150884747505188\n",
      " Epoch : 172 || Loss : 0.9155190587043762\n",
      " Epoch : 173 || Loss : 0.9147573113441467\n",
      " Epoch : 174 || Loss : 0.915083646774292\n",
      " Epoch : 175 || Loss : 0.9145573973655701\n",
      " Epoch : 176 || Loss : 0.9147635102272034\n",
      " Epoch : 177 || Loss : 0.914185106754303\n",
      " Epoch : 178 || Loss : 0.9144488573074341\n",
      " Epoch : 179 || Loss : 0.9139869213104248\n",
      " Epoch : 180 || Loss : 0.9142388105392456\n",
      " Epoch : 181 || Loss : 0.9137098789215088\n",
      " Epoch : 182 || Loss : 0.9139583110809326\n",
      " Epoch : 183 || Loss : 0.913463294506073\n",
      " Epoch : 184 || Loss : 0.9138131141662598\n",
      " Epoch : 185 || Loss : 0.9131826758384705\n",
      " Epoch : 186 || Loss : 0.9134647250175476\n",
      " Epoch : 187 || Loss : 0.9128401875495911\n",
      " Epoch : 188 || Loss : 0.9132068157196045\n",
      " Epoch : 189 || Loss : 0.9126641154289246\n",
      " Epoch : 190 || Loss : 0.9129172563552856\n",
      " Epoch : 191 || Loss : 0.9124574065208435\n",
      " Epoch : 192 || Loss : 0.9126580357551575\n",
      " Epoch : 193 || Loss : 0.9121969938278198\n",
      " Epoch : 194 || Loss : 0.9123663902282715\n",
      " Epoch : 195 || Loss : 0.9118937849998474\n",
      " Epoch : 196 || Loss : 0.912201464176178\n",
      " Epoch : 197 || Loss : 0.9118315577507019\n",
      " Epoch : 198 || Loss : 0.9119710326194763\n",
      " Epoch : 199 || Loss : 0.9115124940872192\n",
      " Epoch : 200 || Loss : 0.9117922782897949\n",
      " Epoch : 201 || Loss : 0.91139155626297\n",
      " Epoch : 202 || Loss : 0.9114928245544434\n",
      " Epoch : 203 || Loss : 0.9110718369483948\n",
      " Epoch : 204 || Loss : 0.9112532734870911\n",
      " Epoch : 205 || Loss : 0.9108133912086487\n",
      " Epoch : 206 || Loss : 0.9111260175704956\n",
      " Epoch : 207 || Loss : 0.9107041954994202\n",
      " Epoch : 208 || Loss : 0.9108355045318604\n",
      " Epoch : 209 || Loss : 0.9105023741722107\n",
      " Epoch : 210 || Loss : 0.9105508923530579\n",
      " Epoch : 211 || Loss : 0.9102281332015991\n",
      " Epoch : 212 || Loss : 0.9102676510810852\n",
      " Epoch : 213 || Loss : 0.9099330902099609\n",
      " Epoch : 214 || Loss : 0.9100221395492554\n",
      " Epoch : 215 || Loss : 0.9098075032234192\n",
      " Epoch : 216 || Loss : 0.9096899628639221\n",
      " Epoch : 217 || Loss : 0.9093411564826965\n",
      " Epoch : 218 || Loss : 0.9092216491699219\n",
      " Epoch : 219 || Loss : 0.9090937376022339\n",
      " Epoch : 220 || Loss : 0.9089727401733398\n",
      " Epoch : 221 || Loss : 0.9088823199272156\n",
      " Epoch : 222 || Loss : 0.908750593662262\n",
      " Epoch : 223 || Loss : 0.9086986184120178\n",
      " Epoch : 224 || Loss : 0.9086468815803528\n",
      " Epoch : 225 || Loss : 0.9084985852241516\n",
      " Epoch : 226 || Loss : 0.9084041714668274\n",
      " Epoch : 227 || Loss : 0.9083189368247986\n",
      " Epoch : 228 || Loss : 0.9081301093101501\n",
      " Epoch : 229 || Loss : 0.9081496596336365\n",
      " Epoch : 230 || Loss : 0.9080711603164673\n",
      " Epoch : 231 || Loss : 0.9082303643226624\n",
      " Epoch : 232 || Loss : 0.9076178073883057\n",
      " Epoch : 233 || Loss : 0.907730221748352\n",
      " Epoch : 234 || Loss : 0.9074376821517944\n",
      " Epoch : 235 || Loss : 0.9073570966720581\n",
      " Epoch : 236 || Loss : 0.9073269367218018\n",
      " Epoch : 237 || Loss : 0.9071400761604309\n",
      " Epoch : 238 || Loss : 0.9070263504981995\n",
      " Epoch : 239 || Loss : 0.9070691466331482\n",
      " Epoch : 240 || Loss : 0.9066554307937622\n",
      " Epoch : 241 || Loss : 0.9070218205451965\n",
      " Epoch : 242 || Loss : 0.9063051342964172\n",
      " Epoch : 243 || Loss : 0.9064669609069824\n",
      " Epoch : 244 || Loss : 0.9060855507850647\n",
      " Epoch : 245 || Loss : 0.9059047698974609\n",
      " Epoch : 246 || Loss : 0.9054818749427795\n",
      " Epoch : 247 || Loss : 0.9055776000022888\n",
      " Epoch : 248 || Loss : 0.9052556157112122\n",
      " Epoch : 249 || Loss : 0.9054062366485596\n",
      " Epoch : 250 || Loss : 0.9048166275024414\n",
      " Epoch : 251 || Loss : 0.9049719572067261\n",
      " Epoch : 252 || Loss : 0.9045611619949341\n",
      " Epoch : 253 || Loss : 0.9046410322189331\n",
      " Epoch : 254 || Loss : 0.9045422077178955\n",
      " Epoch : 255 || Loss : 0.9043743014335632\n",
      " Epoch : 256 || Loss : 0.9041191339492798\n",
      " Epoch : 257 || Loss : 0.9040647149085999\n",
      " Epoch : 258 || Loss : 0.9034903645515442\n",
      " Epoch : 259 || Loss : 0.9036446213722229\n",
      " Epoch : 260 || Loss : 0.9031766653060913\n",
      " Epoch : 261 || Loss : 0.9030405879020691\n",
      " Epoch : 262 || Loss : 0.9026476144790649\n",
      " Epoch : 263 || Loss : 0.9025375247001648\n",
      " Epoch : 264 || Loss : 0.9020310044288635\n",
      " Epoch : 265 || Loss : 0.9020709991455078\n",
      " Epoch : 266 || Loss : 0.9014087319374084\n",
      " Epoch : 267 || Loss : 0.9013304710388184\n",
      " Epoch : 268 || Loss : 0.9010046124458313\n",
      " Epoch : 269 || Loss : 0.90091472864151\n",
      " Epoch : 270 || Loss : 0.9005087614059448\n",
      " Epoch : 271 || Loss : 0.9004178047180176\n",
      " Epoch : 272 || Loss : 0.899796187877655\n",
      " Epoch : 273 || Loss : 0.8996933102607727\n",
      " Epoch : 274 || Loss : 0.899121105670929\n",
      " Epoch : 275 || Loss : 0.8990636467933655\n",
      " Epoch : 276 || Loss : 0.8983545303344727\n",
      " Epoch : 277 || Loss : 0.898224949836731\n",
      " Epoch : 278 || Loss : 0.8976585865020752\n",
      " Epoch : 279 || Loss : 0.8976381421089172\n",
      " Epoch : 280 || Loss : 0.8970036506652832\n",
      " Epoch : 281 || Loss : 0.8969182968139648\n",
      " Epoch : 282 || Loss : 0.8962276577949524\n",
      " Epoch : 283 || Loss : 0.8961333632469177\n",
      " Epoch : 284 || Loss : 0.8952367901802063\n",
      " Epoch : 285 || Loss : 0.895521342754364\n",
      " Epoch : 286 || Loss : 0.8946134448051453\n",
      " Epoch : 287 || Loss : 0.8947604894638062\n",
      " Epoch : 288 || Loss : 0.8938738703727722\n",
      " Epoch : 289 || Loss : 0.893727719783783\n",
      " Epoch : 290 || Loss : 0.8927527666091919\n",
      " Epoch : 291 || Loss : 0.8923588395118713\n",
      " Epoch : 292 || Loss : 0.8914026021957397\n",
      " Epoch : 293 || Loss : 0.8911598920822144\n",
      " Epoch : 294 || Loss : 0.889924168586731\n",
      " Epoch : 295 || Loss : 0.889789879322052\n",
      " Epoch : 296 || Loss : 0.8881068825721741\n",
      " Epoch : 297 || Loss : 0.8880488276481628\n",
      " Epoch : 298 || Loss : 0.886447548866272\n",
      " Epoch : 299 || Loss : 0.8866794109344482\n",
      " Epoch : 300 || Loss : 0.8850204944610596\n",
      " Epoch : 301 || Loss : 0.8849148750305176\n",
      " Epoch : 302 || Loss : 0.88382488489151\n",
      " Epoch : 303 || Loss : 0.8834496736526489\n",
      " Epoch : 304 || Loss : 0.8820157647132874\n",
      " Epoch : 305 || Loss : 0.8819245100021362\n",
      " Epoch : 306 || Loss : 0.8806785941123962\n",
      " Epoch : 307 || Loss : 0.8806391954421997\n",
      " Epoch : 308 || Loss : 0.8796424269676208\n",
      " Epoch : 309 || Loss : 0.8797735571861267\n",
      " Epoch : 310 || Loss : 0.8786056041717529\n",
      " Epoch : 311 || Loss : 0.878233015537262\n",
      " Epoch : 312 || Loss : 0.877459704875946\n",
      " Epoch : 313 || Loss : 0.8769285082817078\n",
      " Epoch : 314 || Loss : 0.8764423727989197\n",
      " Epoch : 315 || Loss : 0.8756186962127686\n",
      " Epoch : 316 || Loss : 0.8749398589134216\n",
      " Epoch : 317 || Loss : 0.874563992023468\n",
      " Epoch : 318 || Loss : 0.8736103177070618\n",
      " Epoch : 319 || Loss : 0.8730066418647766\n",
      " Epoch : 320 || Loss : 0.8725649118423462\n",
      " Epoch : 321 || Loss : 0.8731946349143982\n",
      " Epoch : 322 || Loss : 0.8722337484359741\n",
      " Epoch : 323 || Loss : 0.8717906475067139\n",
      " Epoch : 324 || Loss : 0.87144535779953\n",
      " Epoch : 325 || Loss : 0.8706842660903931\n",
      " Epoch : 326 || Loss : 0.8702378869056702\n",
      " Epoch : 327 || Loss : 0.8697312474250793\n",
      " Epoch : 328 || Loss : 0.8695029020309448\n",
      " Epoch : 329 || Loss : 0.8692607283592224\n",
      " Epoch : 330 || Loss : 0.8689581751823425\n",
      " Epoch : 331 || Loss : 0.8687160611152649\n",
      " Epoch : 332 || Loss : 0.8683151006698608\n",
      " Epoch : 333 || Loss : 0.8679506182670593\n",
      " Epoch : 334 || Loss : 0.8676345944404602\n",
      " Epoch : 335 || Loss : 0.8673428893089294\n",
      " Epoch : 336 || Loss : 0.8668473362922668\n",
      " Epoch : 337 || Loss : 0.8665447235107422\n",
      " Epoch : 338 || Loss : 0.8662270307540894\n",
      " Epoch : 339 || Loss : 0.8657628297805786\n",
      " Epoch : 340 || Loss : 0.8654252886772156\n",
      " Epoch : 341 || Loss : 0.8649014234542847\n",
      " Epoch : 342 || Loss : 0.8646420240402222\n",
      " Epoch : 343 || Loss : 0.8642504215240479\n",
      " Epoch : 344 || Loss : 0.8643019795417786\n",
      " Epoch : 345 || Loss : 0.863868236541748\n",
      " Epoch : 346 || Loss : 0.8637515902519226\n",
      " Epoch : 347 || Loss : 0.8635703325271606\n",
      " Epoch : 348 || Loss : 0.8643848896026611\n",
      " Epoch : 349 || Loss : 0.8631523251533508\n",
      " Epoch : 350 || Loss : 0.8632394671440125\n",
      " Epoch : 351 || Loss : 0.8625993728637695\n",
      " Epoch : 352 || Loss : 0.8621582984924316\n",
      " Epoch : 353 || Loss : 0.8618155717849731\n",
      " Epoch : 354 || Loss : 0.8615451455116272\n",
      " Epoch : 355 || Loss : 0.8613681197166443\n",
      " Epoch : 356 || Loss : 0.8609462380409241\n",
      " Epoch : 357 || Loss : 0.8605982065200806\n",
      " Epoch : 358 || Loss : 0.860383927822113\n",
      " Epoch : 359 || Loss : 0.8605097532272339\n",
      " Epoch : 360 || Loss : 0.8600084185600281\n",
      " Epoch : 361 || Loss : 0.8595572710037231\n",
      " Epoch : 362 || Loss : 0.8593074679374695\n",
      " Epoch : 363 || Loss : 0.8590559363365173\n",
      " Epoch : 364 || Loss : 0.8588528037071228\n",
      " Epoch : 365 || Loss : 0.8588528633117676\n",
      " Epoch : 366 || Loss : 0.8583292365074158\n",
      " Epoch : 367 || Loss : 0.8584856390953064\n",
      " Epoch : 368 || Loss : 0.8579368591308594\n",
      " Epoch : 369 || Loss : 0.8579229116439819\n",
      " Epoch : 370 || Loss : 0.8575171232223511\n",
      " Epoch : 371 || Loss : 0.8575419187545776\n",
      " Epoch : 372 || Loss : 0.8570461273193359\n",
      " Epoch : 373 || Loss : 0.8565284013748169\n",
      " Epoch : 374 || Loss : 0.8565244078636169\n",
      " Epoch : 375 || Loss : 0.8563845753669739\n",
      " Epoch : 376 || Loss : 0.8568612933158875\n",
      " Epoch : 377 || Loss : 0.8562187552452087\n",
      " Epoch : 378 || Loss : 0.8555848002433777\n",
      " Epoch : 379 || Loss : 0.8554094433784485\n",
      " Epoch : 380 || Loss : 0.854829728603363\n",
      " Epoch : 381 || Loss : 0.8550747632980347\n",
      " Epoch : 382 || Loss : 0.854647696018219\n",
      " Epoch : 383 || Loss : 0.8543663024902344\n",
      " Epoch : 384 || Loss : 0.8542822003364563\n",
      " Epoch : 385 || Loss : 0.8537644147872925\n",
      " Epoch : 386 || Loss : 0.8537188172340393\n",
      " Epoch : 387 || Loss : 0.8535979986190796\n",
      " Epoch : 388 || Loss : 0.8530899882316589\n",
      " Epoch : 389 || Loss : 0.8530257344245911\n",
      " Epoch : 390 || Loss : 0.8531284332275391\n",
      " Epoch : 391 || Loss : 0.8529180288314819\n",
      " Epoch : 392 || Loss : 0.8527650237083435\n",
      " Epoch : 393 || Loss : 0.8524554371833801\n",
      " Epoch : 394 || Loss : 0.8518161177635193\n",
      " Epoch : 395 || Loss : 0.8524538278579712\n",
      " Epoch : 396 || Loss : 0.8522226214408875\n",
      " Epoch : 397 || Loss : 0.8514621257781982\n",
      " Epoch : 398 || Loss : 0.8510189652442932\n",
      " Epoch : 399 || Loss : 0.8510518670082092\n",
      " Epoch : 400 || Loss : 0.8506945371627808\n",
      " Epoch : 401 || Loss : 0.8503568172454834\n",
      " Epoch : 402 || Loss : 0.8498901128768921\n",
      " Epoch : 403 || Loss : 0.8501445651054382\n",
      " Epoch : 404 || Loss : 0.8496133089065552\n",
      " Epoch : 405 || Loss : 0.8496799468994141\n",
      " Epoch : 406 || Loss : 0.8496367931365967\n",
      " Epoch : 407 || Loss : 0.8498539328575134\n",
      " Epoch : 408 || Loss : 0.8499813675880432\n",
      " Epoch : 409 || Loss : 0.8489516973495483\n",
      " Epoch : 410 || Loss : 0.8484256863594055\n",
      " Epoch : 411 || Loss : 0.8482587933540344\n",
      " Epoch : 412 || Loss : 0.8478424549102783\n",
      " Epoch : 413 || Loss : 0.8476439714431763\n",
      " Epoch : 414 || Loss : 0.8471476435661316\n",
      " Epoch : 415 || Loss : 0.8473693132400513\n",
      " Epoch : 416 || Loss : 0.8470836877822876\n",
      " Epoch : 417 || Loss : 0.8482515811920166\n",
      " Epoch : 418 || Loss : 0.8525996804237366\n",
      " Epoch : 419 || Loss : 0.8476842045783997\n",
      " Epoch : 420 || Loss : 0.8465821743011475\n",
      " Epoch : 421 || Loss : 0.8457899689674377\n",
      " Epoch : 422 || Loss : 0.845537006855011\n",
      " Epoch : 423 || Loss : 0.8452405333518982\n",
      " Epoch : 424 || Loss : 0.8451465964317322\n",
      " Epoch : 425 || Loss : 0.844761848449707\n",
      " Epoch : 426 || Loss : 0.844674289226532\n",
      " Epoch : 427 || Loss : 0.844359278678894\n",
      " Epoch : 428 || Loss : 0.844425618648529\n",
      " Epoch : 429 || Loss : 0.843690037727356\n",
      " Epoch : 430 || Loss : 0.8436821103096008\n",
      " Epoch : 431 || Loss : 0.843542754650116\n",
      " Epoch : 432 || Loss : 0.8433135747909546\n",
      " Epoch : 433 || Loss : 0.8428399562835693\n",
      " Epoch : 434 || Loss : 0.8426374793052673\n",
      " Epoch : 435 || Loss : 0.8423026204109192\n",
      " Epoch : 436 || Loss : 0.8421209454536438\n",
      " Epoch : 437 || Loss : 0.8420618176460266\n",
      " Epoch : 438 || Loss : 0.8418856859207153\n",
      " Epoch : 439 || Loss : 0.8412530422210693\n",
      " Epoch : 440 || Loss : 0.8412114381790161\n",
      " Epoch : 441 || Loss : 0.8406457901000977\n",
      " Epoch : 442 || Loss : 0.8407706618309021\n",
      " Epoch : 443 || Loss : 0.8403310775756836\n",
      " Epoch : 444 || Loss : 0.840441107749939\n",
      " Epoch : 445 || Loss : 0.8401421904563904\n",
      " Epoch : 446 || Loss : 0.8400655388832092\n",
      " Epoch : 447 || Loss : 0.8397127985954285\n",
      " Epoch : 448 || Loss : 0.8396793603897095\n",
      " Epoch : 449 || Loss : 0.8391891121864319\n",
      " Epoch : 450 || Loss : 0.8394894003868103\n",
      " Epoch : 451 || Loss : 0.8387781381607056\n",
      " Epoch : 452 || Loss : 0.8390344381332397\n",
      " Epoch : 453 || Loss : 0.8383540511131287\n",
      " Epoch : 454 || Loss : 0.8380975723266602\n",
      " Epoch : 455 || Loss : 0.8378214836120605\n",
      " Epoch : 456 || Loss : 0.8378318548202515\n",
      " Epoch : 457 || Loss : 0.8375172019004822\n",
      " Epoch : 458 || Loss : 0.8378320932388306\n",
      " Epoch : 459 || Loss : 0.8370936512947083\n",
      " Epoch : 460 || Loss : 0.8370543122291565\n",
      " Epoch : 461 || Loss : 0.8362451791763306\n",
      " Epoch : 462 || Loss : 0.8365926146507263\n",
      " Epoch : 463 || Loss : 0.835933268070221\n",
      " Epoch : 464 || Loss : 0.8358994126319885\n",
      " Epoch : 465 || Loss : 0.8350414037704468\n",
      " Epoch : 466 || Loss : 0.8352723717689514\n",
      " Epoch : 467 || Loss : 0.835084080696106\n",
      " Epoch : 468 || Loss : 0.8348218202590942\n",
      " Epoch : 469 || Loss : 0.8345912098884583\n",
      " Epoch : 470 || Loss : 0.8345704078674316\n",
      " Epoch : 471 || Loss : 0.8339210748672485\n",
      " Epoch : 472 || Loss : 0.833992063999176\n",
      " Epoch : 473 || Loss : 0.8339001536369324\n",
      " Epoch : 474 || Loss : 0.8336496949195862\n",
      " Epoch : 475 || Loss : 0.8332233428955078\n",
      " Epoch : 476 || Loss : 0.8334481716156006\n",
      " Epoch : 477 || Loss : 0.8340958952903748\n",
      " Epoch : 478 || Loss : 0.8339109420776367\n",
      " Epoch : 479 || Loss : 0.8325546383857727\n",
      " Epoch : 480 || Loss : 0.8320235013961792\n",
      " Epoch : 481 || Loss : 0.8316662311553955\n",
      " Epoch : 482 || Loss : 0.8318032622337341\n",
      " Epoch : 483 || Loss : 0.8311045169830322\n",
      " Epoch : 484 || Loss : 0.8311591148376465\n",
      " Epoch : 485 || Loss : 0.830833375453949\n",
      " Epoch : 486 || Loss : 0.8306442499160767\n",
      " Epoch : 487 || Loss : 0.8304203748703003\n",
      " Epoch : 488 || Loss : 0.8302519917488098\n",
      " Epoch : 489 || Loss : 0.8298978805541992\n",
      " Epoch : 490 || Loss : 0.8302196860313416\n",
      " Epoch : 491 || Loss : 0.8296352624893188\n",
      " Epoch : 492 || Loss : 0.8292847871780396\n",
      " Epoch : 493 || Loss : 0.8289983868598938\n",
      " Epoch : 494 || Loss : 0.8290398716926575\n",
      " Epoch : 495 || Loss : 0.8284932971000671\n",
      " Epoch : 496 || Loss : 0.828453540802002\n",
      " Epoch : 497 || Loss : 0.8280574679374695\n",
      " Epoch : 498 || Loss : 0.828111469745636\n",
      " Epoch : 499 || Loss : 0.8274917006492615\n",
      " Epoch : 500 || Loss : 0.827357828617096\n"
     ]
    }
   ],
   "source": [
    "epoch = 500\n",
    "for i in range(0, epoch):\n",
    "    train_loss = 0\n",
    "    cnt = 0.\n",
    "    for id_user in range(nb_users):\n",
    "        input = Variable(train[id_user]).unsqueeze(0) # for creating a batch(2D) of 1 input vector and change it dimension \n",
    "        target = input.clone()\n",
    "        if torch.sum(target.data > 0) > 0: # considering only users that rated atleast one movie\n",
    "            output = ae.forward(input)\n",
    "            target.requires_grad = False # don't compute the gradient wrt target\n",
    "            output[target == 0] = 0\n",
    "            loss = criterion(output , target)\n",
    "            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10) # only consider rated movies(viewed by user)\n",
    "            loss.backward() # backward decide the direction of step to which the weights are update \n",
    "            train_loss += np.sqrt(loss.data*mean_corrector)\n",
    "            cnt += 1.\n",
    "            optimizer.step() # decide the value by which the update happens \n",
    "    print(f\" Epoch : {i+1} || Loss : {train_loss/cnt}\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 0.9549281597137451\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "cnt  = 0\n",
    "for id_user in range(nb_users):\n",
    "    input = Variable(train[id_user]).unsqueeze(0)\n",
    "    target = Variable(test[id_user]).unsqueeze(0)\n",
    "    if torch.sum(target.data > 0) > 0: # considering only users that rated atleast one movie\n",
    "        output = ae.forward(input)\n",
    "        target.requires_grad = False # don't compute the gradient wrt target\n",
    "        output[target == 0] = 0\n",
    "        loss = criterion(output , target)\n",
    "        mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "        test_loss += np.sqrt(loss.data*mean_corrector)\n",
    "        cnt += 1.\n",
    "print(f\"Loss : {test_loss/cnt}\")     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
